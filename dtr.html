---
layout: default
title: Dynamic Tensor Rematerialization
---
Dynamic Tensor Rematerialization <br>

Authors: Marisa Kirisame, Steven Lyubomirsky, Altan Haan, Jennifer Brennan, Mike He, Jared Roesch, Tianqi Chen, Zachary Tatlock <br>

<a href="https://arxiv.org/abs/2006.09616">paper</a> <a href="resources/DTR.pptx">slides</a> <a href="https://www.youtube.com/watch?v=S9KJ37Sx2XY">video</a> <br>

DTR save GPU memory for DNN training by recomputing result when needed. <br>

It allow a 3x memory saving for 20% extra compute time. <br>

It is also simple and can be easily incorporated into different deep learning framework. If you are implementing one, give it a shot! <br> 

Gradient Checkpointing is a technique to save memory for Deep Neural Network training, or more generally, for reverse-mode automatic differentiation. <br>

As memory planning is NP-complete, and as gradient checkpointing has to plan memory for arbitary program with control flow, previous work made different restriction which sacrifice performance or usability. <br>

Some works models the program as a stack machine with no heap, and suffers performance degradtion when the assumption is broken (for example, NN with highway connection/branching). <br>

Other works use an ILP solver, which consume lots of time to find the optimal memory planning, and can only be used for program without control flow, posing problem for real world adoption. <br>

Additionally, gradient checkpointing couple the concept of calculating derivatives, with that of saving memory by recomputing, which add complexity and limit applications range. <br>

DTR tackle the problems above by planning the memory greedily at runtime, with a Tensor-Level Cache, instead of as a compiler pass. <br>

Adoption: <br>
<a href="https://github.com/MegEngine/MegEngine/wiki/Reduce-GPU-memory-usage-by-Dynamic-Tensor-Rematerialization">MegEngine</a> <br>
