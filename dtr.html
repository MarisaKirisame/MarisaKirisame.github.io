---
layout: default
title: Dynamic Tensor Rematerialization
---
<h1>Dynamic Tensor Rematerialization(DTR)</h1>

<div> <a href="https://arxiv.org/abs/2006.09616">paper</a> <a href="resources/DTR.pptx">slides</a> <a href="https://www.youtube.com/watch?v=S9KJ37Sx2XY">video</a> </div>

<div style="color:#222;font-size:0.8rem">Marisa Kirisame, Steven Lyubomirsky, Altan Haan, Jennifer Brennan, Mike He, Jared Roesch, Tianqi Chen, Zachary Tatlock </div>

<br>

<h2>DTR:</h2>

<ul>
  <li>Save memory for NN by dynamically discarding and recomputing intermediate result at runtime.</li>
  <li>By being smart about what to keep and discard, let you train larger model under tight budget.</li>
  <li>Save 3x memory for 20% extra compute time!</li>
  <li>Make no assumptions about the model, and does not require a static graph.</li>
  <li>So it can be easily implemented into different deep learning framework.</li>
  <li>Still achieve amazing space-time tradeoff!</li>
</ul> 
     
<h2>Technical Dive</h2>
<ol start="0">
  <li>Gradient Checkpointing is a technique to save memory for Deep Neural Network training.</li>
  <li>Or more generally, for reverse-mode automatic differentiation.</li>
  <li>However, memory planning is np-complete.</li>
  <li>Checkpointing also has to deal with program with arbitary control flow.</li>
  <li>To combat this, previous work made different restriction which sacrifice performance or usability.</li>
  <li>Some works models the program as a stack machine with no heap...</li>
  <li>And suffers performance degradtion when the assumption is broken!</li>
  <li>(For example, NN with highway connection/branching).</li>
  <li>Other works use an ILP solver, which consume lots of time to find the optimal memory planning.</li>
  <li>And can only be used for program/framework without control flow, posing problem for real world adoption.</li>
  <li>Additionally, gradient checkpointing couple derivative calculation, with memory saving by recomputing.</li>
  <li>This add complexity and limit applications range.</li>
  <li>DTR tackles the problems above by planning the memory greedily at runtime, instead of as a compiler pass.</li>
  <li>This solves the control flow and stack machine issue, as we do not model the program in anyway!</li>
  <li>However, with a novel cache eviction policy, we are still able to achieve great performance.</li>
</ol>

<h2>Adoption:</h2>
<a href="https://github.com/MegEngine/MegEngine/wiki/Reduce-GPU-memory-usage-by-Dynamic-Tensor-Rematerialization">MegEngine</a> <br>
